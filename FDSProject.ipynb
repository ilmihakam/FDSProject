{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib as mlt\n","import seaborn as sb\n","import numpy as np"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
# Path to the large CSV file
large_csv_path = 'path/to/TWO_CENTURIES_OF_UM_RACES.csv'

# Number of rows per chunk
chunk_size = 100000  # Adjust as needed

# Read the large CSV file into a DataFrame
df = pd.read_csv(large_csv_path)

# Determine the total number of chunks
num_chunks = len(df) // chunk_size + 1

# Split the DataFrame into smaller chunks
chunks = [df[i*chunk_size:(i+1)*chunk_size] for i in range(num_chunks)]

# Write each chunk to a separate CSV file
for i, chunk in enumerate(chunks):
    chunk.to_csv(f'chunk_{i+1}.csv', index=False)
